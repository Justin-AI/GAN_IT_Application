{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAN_Example.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HauM-62u8Bel","colab_type":"text"},"source":["**Note:**\n","<br>Set **SkipTraining = 1** to skip the GAN training code in this notebook and instead import images of training results.\n","\n","<br>You can click on a code cell and hit **ctrl+enter** to run just that cell. Alternatively, you can rull all code cells by hitting **ctrl+F9**"]},{"cell_type":"code","metadata":{"id":"hrxzoqBZ78pp","colab_type":"code","colab":{}},"source":["SkipTraining = 0 # 1 = skip code, 0 = run code"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CqnN10TV8CGK","colab_type":"text"},"source":["# Generative Adversarial Networks: Example Implementation (Vanilla Structure)"]},{"cell_type":"markdown","metadata":{"id":"rt8sT8lT8nQX","colab_type":"text"},"source":["##Import Required Python Libraries"]},{"cell_type":"markdown","metadata":{"id":"8ndcWzYe9UIP","colab_type":"text"},"source":["The below code cell imports modules from several Python libraries which will be used for our analysis.\n","\n","**Numpy** - an efficient library for array operations\n","<br>**Keras** - a high-level machine learning library that uses the Tensorflow backend (compuational advantages)\n","<br>**Matplotlib** - a plotting library to help with visualisations\n","<br>**SciPy** - a library for scientific computing. In our case we are just using it to quickly normalise some arrays.\n","<br>"]},{"cell_type":"code","metadata":{"id":"uXe9Hzuf9U_4","colab_type":"code","colab":{}},"source":["from numpy import loadtxt\n","from numpy import hstack\n","from numpy import zeros\n","from numpy import ones\n","from numpy import append\n","import numpy as np\n","from numpy.random import rand\n","from numpy.random import randn\n","from numpy.random import seed\n","from numpy.random import shuffle\n","from keras.models import Sequential \n","from keras.layers import Dense\n","from keras.optimizers import Adam\n","from keras.layers import LeakyReLU\n","from matplotlib import pyplot\n","from scipy import stats\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DOUCRkRaAa2G","colab_type":"text"},"source":["##Create Simulated \"Real\" Data for our Example"]},{"cell_type":"markdown","metadata":{"id":"6Brvcf15Arh5","colab_type":"text"},"source":["For our analysis of insider trading data we want to prove that a GAN is an appropriate ML technique for anomaly detection in the contextual setting.\n","\n","<br>To prove that GANs can be trained to learn representations of normal data and subsequently flag anomalies we will create some clean data for model training. Once the model is trained we can create another set of data which is from the same distribution and then randomly inject some anomalies. If the GAN performs as expected, it will be able to identify the anomalous data.\n","\n","<br>For our simulated data, lets assume that there are three different variables of interest. We could imagine that they represent features such as Price Impulse, Trade Size and Relative Trade Volume (10 prior vs. 100 prior orders).\n","\n","<br>We will also add a final column vector of ones which indicates that the observations represent \"real\" data"]},{"cell_type":"code","metadata":{"id":"UauZKN3uArAE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"596f9fd6-10dd-4f63-d29b-b829c2f15edb","executionInfo":{"status":"ok","timestamp":1571600773591,"user_tz":-660,"elapsed":1283,"user":{"displayName":"Justin Hitchen","photoUrl":"","userId":"15321264705279366966"}}},"source":["# Lets first set the same random seed to facilitate the reproducibility of our work\n","seed(0)\n","\n","# Simulating some clean data\n","\n","A = 0.95 + 6*rand(100000,1)/100 # Let's set the first variable as random uniformly distributed data between 0.95 and 1.01. This reflects that bids/asks are often placed close to the best price.\n","B = 100 + 20*randn(100000,1) # Let's set trade sizes as normally distributed with a mean trade size of 100 parcels and a high variability\n","C = 1 + randn(100000,1)/100 # Let's set relative trade volume as normally distributed around one. This reflects that there is no clear pattern in relative trade volume during normal trading.\n","D = ones((100000,1)) # 1 values to indicate real samples\n","\n","SimData=np.concatenate((A,B,C,D), axis=1)\n","\n","# Let's also inspect some of the simulated data\n","np.set_printoptions(precision=4)\n","np.set_printoptions(suppress=True)\n","print(SimData[0:10,:])\n","\n","# Let's use the first 70% of our simulated data for GAN training and save the remainder for mixing with anomalies\n","\n","MixedData = SimData[70000:,:] # Separate the last 3k observations for later use\n","MixedData[:,3]=0 # The Mixed Data is only used in the deployment phase. In the deployment phase 0 will indicate normal, 1 will indicate anomaly\n","SimData = SimData[0:70000,:] # Separate the first 7k observations for model training\n","\n","SimData.shape"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[[  0.9829 118.2301   0.9933   1.    ]\n"," [  0.9929 101.5908   0.9863   1.    ]\n"," [  0.9862  81.0139   0.985    1.    ]\n"," [  0.9827 100.114    0.9986   1.    ]\n"," [  0.9754 136.6368   0.9931   1.    ]\n"," [  0.9888 104.784    0.9998   1.    ]\n"," [  0.9763  94.0114   0.9993   1.    ]\n"," [  1.0035  97.509    1.006    1.    ]\n"," [  1.0078 106.0129   1.0013   1.    ]\n"," [  0.973   91.9012   1.0137   1.    ]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(70000, 4)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"L_Ba8_MJByF0","colab_type":"text"},"source":["##Inserting Anomalies into the Data Set"]},{"cell_type":"markdown","metadata":{"id":"Fz61rVkkCwRt","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"_VOKZW8rCw5N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"outputId":"ea60d9b7-d435-4a10-e890-277819fe3b08","executionInfo":{"status":"ok","timestamp":1571600773903,"user_tz":-660,"elapsed":1571,"user":{"displayName":"Justin Hitchen","photoUrl":"","userId":"15321264705279366966"}}},"source":["\n","# Let's generate 20 anomalies to represent insider trading\n","\n","\n","E = 0.99 + 3*rand(20,1)/100 # Let's set the first variable as random uniformly distributed data between 0.99 and 1.02. This reflects that the insider trader has a sense of urgency in placing their orders.\n","F = 30 + 30*rand(20,1) # Let's set the second variable as random uniformly distributed data between 30 and 60. This reflects that the insider trader is attempting to place multiple smaller orders to avoid detection.\n","G = 0.95 + randn(20,1)/100 # Let's set relative trade volume as normally distributed around 0.95. This reflects that the insider's orders are impacting relative trade volume as they attempt to disguise their orders across multiple smaller trades.\n","H = ones((20,1)) # We will label these data points as the anomaly data points so we can check whether the model correctly identified\n","\n","ITData=np.concatenate((E,F,G,H), axis=1)\n","\n","# Let's also inspect some of the simulated anomaly data\n","np.set_printoptions(precision=4)\n","np.set_printoptions(suppress=True)\n","print(\"Sample IT Data\")\n","print(ITData[0:10,:])\n","\n","# Let's insert the anomalies into our mixed data and then shuffle the data\n","MixedData=np.concatenate((MixedData,ITData), axis=0)\n","shuffle(MixedData)\n","\n","print(\"Sample Mixed Data\")\n","print(MixedData[0:10,:])\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Sample IT Data\n","[[ 1.0126 37.3985  0.9592  1.    ]\n"," [ 1.0101 59.5441  0.9414  1.    ]\n"," [ 1.0129 59.9846  0.9358  1.    ]\n"," [ 1.0088 53.5655  0.9614  1.    ]\n"," [ 0.9933 34.3073  0.9403  1.    ]\n"," [ 0.9978 32.3941  0.9579  1.    ]\n"," [ 0.9939 53.5999  0.9669  1.    ]\n"," [ 0.9943 38.9223  0.9503  1.    ]\n"," [ 1.0106 35.5868  0.9628  1.    ]\n"," [ 1.0076 57.9353  0.9439  1.    ]]\n","Sample Mixed Data\n","[[  0.9614  99.1899   1.011    0.    ]\n"," [  0.968  118.891    0.9908   0.    ]\n"," [  0.9961  71.2588   1.0127   0.    ]\n"," [  1.0062 130.8594   1.0148   0.    ]\n"," [  1.0087  97.3095   1.0147   0.    ]\n"," [  0.9708  61.2612   0.9951   0.    ]\n"," [  0.9655 123.899    0.9948   0.    ]\n"," [  1.0085 101.5095   0.9879   0.    ]\n"," [  0.9784  74.8265   0.9981   0.    ]\n"," [  0.9959  77.0031   1.0093   0.    ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G3pJ1oU-IH78","colab_type":"text"},"source":["## Normalise Data"]},{"cell_type":"markdown","metadata":{"id":"1-2l5u7NIOXY","colab_type":"text"},"source":["Normalising each of the variables will facilitate model training."]},{"cell_type":"code","metadata":{"id":"rO_Y8a0-IO_d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":782},"outputId":"eedb4c8f-c7b7-47cb-ef42-3f9cfdf9ff09","executionInfo":{"status":"ok","timestamp":1571600773904,"user_tz":-660,"elapsed":1530,"user":{"displayName":"Justin Hitchen","photoUrl":"","userId":"15321264705279366966"}}},"source":["# Temporarily append data to calculate z-score across\n","AllData = np.concatenate((SimData,MixedData),axis=0)\n","NormAllData = stats.zscore(AllData[:,0:3], axis=0)\n","NormAllData = np.concatenate((NormAllData,AllData[:,3].reshape(-1,1)), axis=1) # leave final column as 1/0 values\n","\n","\n","#NormSimData = stats.zscore(SimData[:,0:3], axis=0) # calculate z-scores of each observation relative to the rest of the column\n","#NormSimData=np.concatenate((NormSimData,SimData[:,3].reshape(-1,1)), axis=1) # leave final column as 1/0 values\n","NormSimData = NormAllData[0:70000,:]\n","\n","#NormMixedData = stats.zscore(MixedData[:,0:3], axis=0) # calculate z-scores of each observation relative to the rest of the column\n","#NormMixedData=np.concatenate((NormMixedData,MixedData[:,3].reshape(-1,1)), axis=1) # leave final column as 1/0 values\n","NormMixedData = NormAllData[70000:,:]\n","\n","\n","# Let's check a sample of the normalised data\n","np.set_printoptions(precision=4)\n","np.set_printoptions(suppress=True)\n","print(\"Normalised Simulated 'Real' Data\")\n","print(NormSimData[0:10,:])\n","print(\"\")\n","print(\"Normalised Mixed Data\")\n","print(NormMixedData[0:10,:])\n","print(\"\")\n","print(\"ITs in the Mixed Data\")\n","print(NormMixedData[NormMixedData[:,3]==1])\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Normalised Simulated 'Real' Data\n","[[ 0.1703  0.9028 -0.6624  1.    ]\n"," [ 0.7458  0.0697 -1.3662  1.    ]\n"," [ 0.3569 -0.9604 -1.4951  1.    ]\n"," [ 0.1567 -0.0042 -0.1335  1.    ]\n"," [-0.2627  1.8243 -0.6815  1.    ]\n"," [ 0.5061  0.2296 -0.0135  1.    ]\n"," [-0.2145 -0.3097 -0.0636  1.    ]\n"," [ 1.3567 -0.1346  0.6014  1.    ]\n"," [ 1.6054  0.2911  0.1351  1.    ]\n"," [-0.4018 -0.4154  1.373   1.    ]]\n","\n","Normalised Mixed Data\n","[[-1.0696 -0.0505  1.1075  0.    ]\n"," [-0.69    0.9359 -0.9113  0.    ]\n"," [ 0.931  -1.4488  1.268   0.    ]\n"," [ 1.5132  1.5351  1.4819  0.    ]\n"," [ 1.6541 -0.1446  1.47    0.    ]\n"," [-0.5298 -1.9494 -0.4865  0.    ]\n"," [-0.8342  1.1866 -0.5099  0.    ]\n"," [ 1.6437  0.0657 -1.2038  0.    ]\n"," [-0.091  -1.2702 -0.184   0.    ]\n"," [ 0.9185 -1.1612  0.9327  0.    ]]\n","\n","ITs in the Mixed Data\n","[[ 1.8988 -2.0133 -6.4075  1.    ]\n"," [ 0.8004 -2.3329 -3.3019  1.    ]\n"," [ 1.8345 -2.7904 -3.3928  1.    ]\n"," [ 0.8285 -3.0678 -4.9564  1.    ]\n"," [ 1.6873 -3.1733 -4.9445  1.    ]\n"," [ 1.7389 -2.0353 -5.8463  1.    ]\n"," [ 1.586  -2.0659 -5.525   1.    ]\n"," [ 1.2092 -2.2885 -3.4234  1.    ]\n"," [ 1.6011 -2.7204 -4.9809  1.    ]\n"," [ 1.8794 -3.1441 -4.0653  1.    ]\n"," [ 1.7673 -3.2348 -3.7042  1.    ]\n"," [ 2.1788 -2.5849 -4.4582  1.    ]\n"," [ 1.9165 -2.5302 -5.025   1.    ]\n"," [ 0.6575 -3.3514 -4.7182  1.    ]\n"," [ 1.6646 -2.3347 -3.8442  1.    ]\n"," [ 1.2889 -3.2716 -5.3233  1.    ]\n"," [ 0.7657 -3.2988 -5.9558  1.    ]\n"," [ 1.5908 -2.1159 -5.5947  1.    ]\n"," [ 1.8423 -3.4224 -4.1412  1.    ]\n"," [ 1.0274 -3.3946 -4.2003  1.    ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"idT110UCZucw","colab_type":"text"},"source":["##Define our Discriminator Model"]},{"cell_type":"markdown","metadata":{"id":"00kOviZMZ6KT","colab_type":"text"},"source":["The below code defines a function for our discriminative model within the GAN framework. For this example we are using a standard neural network **(NN)** with 3 input nodes, 2 hidden layers of 50 nodes, and a single node output layer (real/fake predictions).\n","\n","<br>The code has been implemented within Keras, which enables a high level of customisation for the NN layers. As a result, we could choose to have non-uniform hidden layers and develop far more sophisticated versions of the NN used for the discriminative model.\n","\n","<br>***Typical Keras ML Model Structure***\n","<br>1. Define the model as sequential or ___\n","<br>2. "]},{"cell_type":"code","metadata":{"id":"gNoaXgU6a62l","colab_type":"code","colab":{}},"source":["def define_discriminator(n_inputs,Learn_Rate):\n","    model = Sequential()\n","    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n","    model.add(LeakyReLU(0.2))\n","    model.add(Dense(50, activation='relu'))\n","    model.add(LeakyReLU(0.2))\n","    model.add(Dense(1, activation='sigmoid'))\n","    # compile model\n","    opt = Adam(lr=Learn_Rate) # I reduced the learning rate to improve training\n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gSAxPTRbbP_j","colab_type":"text"},"source":["##Define our Generative Model"]},{"cell_type":"markdown","metadata":{"id":"bm1mgQ1_cWV9","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"UBe3Ja-EcW2e","colab_type":"code","colab":{}},"source":["def define_generator(latent_dim, n_outputs):\n","    model = Sequential()\n","    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n","    model.add(LeakyReLU(0.2))\n","    model.add(Dense(50, activation='relu'))\n","    model.add(LeakyReLU(0.2))\n","    model.add(Dense(n_outputs, activation='linear'))\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a5qjB1dncXhV","colab_type":"text"},"source":["##Define our Consolidated GAN Structure"]},{"cell_type":"markdown","metadata":{"id":"XRQxuCaTcZ9o","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"BbcQLQLIcYEf","colab_type":"code","colab":{}},"source":["def define_gan(generator, discriminator,Learn_Rate):\n","  # make weights in the discriminator not trainable\n","  discriminator.trainable = False\n","  # connect them\n","  model = Sequential()\n","  # add generator\n","  model.add(generator)\n","  # add the discriminator\n","  model.add(discriminator)\n","  # compile model\n","  opt = Adam(lr=Learn_Rate) # I reduced the learning rate to improve training\n","  model.compile(loss='binary_crossentropy', optimizer=opt)\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vkF26AdIcajH","colab_type":"text"},"source":["##Define Function to Generate Latent Data Points"]},{"cell_type":"markdown","metadata":{"id":"sPFdaltAca_F","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"4r7_ja_LcboG","colab_type":"code","colab":{}},"source":["# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n=10000):\n","\t# generate points in the latent space\n","\tx_input = rand(latent_dim * n)# testing uniform instead. random normal is typically advised\n","\t# reshape into a batch of inputs for the network\n","\tx_input = x_input.reshape(n, latent_dim)\n","\treturn x_input"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KIc5MjrdGlZ","colab_type":"text"},"source":["##Instruct Generator to Generate New Samples Based on Latent Data Feed"]},{"cell_type":"markdown","metadata":{"id":"xrTa__7FdHEi","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"HJEjsLZ6dPxW","colab_type":"code","colab":{}},"source":["# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, latent_dim, n=10000):\n","\t# generate points in latent space\n","\tx_input = generate_latent_points(latent_dim, n)\n","\t# predict outputs\n","\tX = generator.predict(x_input)\n","\t# create class labels\n","\ty = zeros((n, 1))\n","\treturn X, y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tgg55FLfAwE5","colab_type":"text"},"source":["##Define Performance Evaluation Function"]},{"cell_type":"markdown","metadata":{"id":"3__2l4UQAv8G","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"Js5escFvAvmj","colab_type":"code","colab":{}},"source":["def summarize_performance(epoch, generator, discriminator, real_data, latent_dim, n=100):\n","    # prepare real samples\n","    NumVars = real_data.shape[1]\n","    x_real, y_real = real_data[:,0:NumVars], real_data[:,-1] \n","    # evaluate discriminator on real examples\n","    _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n","    # prepare fake examples\n","    x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n","    # evaluate discriminator on fake examples\n","    _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n","    # summarize discriminator performance\n","    print(\"Epoch: \", epoch,\" D_real_acc: \", acc_real, \" D_fake_acc: \", acc_fake)\n","    # scatter plot real and fake data points\n","    pyplot.figure()\n","    pyplot.title(\"Var1 vs. Var2\")\n","    pyplot.scatter(x_real[:, 0], x_real[:, 1], color='red')\n","    pyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n","    pyplot.show()\n","    \n","    pyplot.figure()\n","    pyplot.title(\"Var1 vs. Var3\")\n","    pyplot.scatter(x_real[:, 0], x_real[:, 2], color='red')\n","    pyplot.scatter(x_fake[:, 0], x_fake[:, 2], color='blue')\n","    pyplot.show()\n","    \n","    pyplot.figure()\n","    pyplot.title(\"Var2 vs. Var3\")\n","    pyplot.scatter(x_real[:, 1], x_real[:, 2], color='red')\n","    pyplot.scatter(x_fake[:, 1], x_fake[:, 2], color='blue')\n","    pyplot.show()\n","\n","    print(\"\")\n","    print(\"\")\n","    print(\"\")\n","    print(\"\")\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VK3aA4F2dQIP","colab_type":"text"},"source":["##Define GAN Training Function"]},{"cell_type":"markdown","metadata":{"id":"OB6kENj5dQhS","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"2m_HyBfLdQ3-","colab_type":"code","colab":{}},"source":["def train(g_model, d_model, gan_model, real_data, latent_dim, n_epochs=10000, n_batch=32, n_eval=2000):\n","  # determine half the size of one batch, for updating the discriminator\n","  half_batch = int(n_batch / 2) \n","  Cum_real = []\n","  Cum_fake = []\n","  Cum_epoch = []\n","  # manually enumerate epochs\n","  for i in range(n_epochs):\n","    # prepare real samples\n","    NumVars = real_data.shape[1]\n","    x_real, y_real = real_data[:,0:NumVars], real_data[:,-1]\n","    # prepare fake examples\n","    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","    # update discriminator\n","    d_model.train_on_batch(x_real, y_real)\n","    d_model.train_on_batch(x_fake, y_fake)\n","    # prepare points in latent space as input for the generator\n","    x_gan = generate_latent_points(latent_dim, n_batch)\n","    # create inverted labels for the fake samples\n","    y_gan = ones((n_batch, 1))\n","    # update the generator via the discriminator's error\n","    gan_model.train_on_batch(x_gan, y_gan)\n","    # evaluate the model every n_eval epochs\n","    if (i+1) % n_eval == 0 or i==0:\n","      summarize_performance(i+1, g_model, d_model, real_data, latent_dim)\n","      _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n","      _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n","      Cum_real = append(Cum_real,acc_real)\n","      Cum_fake = append(Cum_fake,acc_fake)\n","      Cum_epoch = append(Cum_epoch,(i+1))\n","      # Plot historical training performance\n","      pyplot.figure()\n","      pyplot.title(\"Historical Training Performance\")\n","      pyplot.plot(Cum_epoch, Cum_real,'r')\n","      pyplot.plot(Cum_epoch, Cum_fake,'b')\n","      pyplot.show()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ZKK5vByddTv","colab_type":"text"},"source":["##Run our Functions and Evaluate Performance"]},{"cell_type":"markdown","metadata":{"id":"V4nioFIGdd0o","colab_type":"text"},"source":["A perfectly trained GAN would show a discriminator accuracy of approximately 50% for both real predictions and fake predictions during training. This shows that the generative model is producing perfectly indistinguishable synthetic observations. Conversely, the discriminator can also not make any further improvements - the generative model and discriminative model have arrived at a Nash Equilibrium.\n","\n","<br>There are a few parameters that we can play around with for the training process such as *epochs* and *batch size*.\n","\n","<br>**learning rate** - The learning rate controls how quickly the model is adapted to the problem. \n","<br>**epochs** - defines the number times that the learning algorithm will work through the entire training dataset.\n","<br>**batch size** - refers to the number of samples processed before the model is updated.\n","\n","<br>***Choosing the learning rate***\n","<br>Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.\n","\n","<br>A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck. Setting an appropriate learning rate for a GAN is typically a process of trial and error. \n","\n","<br>***Background on setting batch sizes***\n","<br>The three common approaches are outlined below:\n","<br>**Batch Gradient Descent**: Batch Size = Size of Training Set\n","<br>**Stochastic Gradient Descent**: Batch Size = 1\n","<br>**Mini-Batch Gradient Descent**: 1 < Batch Size < Size of Training Set\n","\n","<br>Mini-Batch Gradient Descent is the most common approach in deep learning and is therefore what we have used for our example. The key benefits and downsides of this approach are summarised below.\n","\n","<br>**Benefits**\n","* The model update frequency is higher than batch gradient descent which allows for a more robust convergence, avoiding local minima.\n","* The batched updates provide a computationally more efficient process than stochastic gradient descent.\n","* The batching allows both the efficiency of not having all training data in memory and algorithm implementations.\n","\n","<br>**Downside**\n","* Error information must be accumulated across mini-batches of training examples like batch gradient descent.\n","\n","<br>***So what is the optimal batch size?***\n","<br>Batch sizes are often tuned to an aspect of the computational architecture on which the implementation is being executed. Such as a power of two that fits the memory requirements of the GPU or CPU hardware like 32, 64, 128, 256, etc.\n","<br>A batch size of 32 appears to be most strongly supported by machine learning literature:\n","* \"Practical recommendations for gradient-based training of deep architectures, 2012\"\n","* \"Revisiting Small Batch Training for Deep Neural Networks, 2018.\"\n","<br>"]},{"cell_type":"code","metadata":{"id":"Ce43NxIVdh1g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1PQN5178h5AFECS6MjCNzO5boLq0Im3ZG"},"outputId":"b990fed3-6976-4aa7-85e3-861438ff5164","executionInfo":{"status":"ok","timestamp":1571605467029,"user_tz":-660,"elapsed":3217859,"user":{"displayName":"Justin Hitchen","photoUrl":"","userId":"15321264705279366966"}}},"source":["epochs = 30000\n","batches = 64\n","AdamlearnRate = 0.001 # default is 0.001, 0.0005 works ok\n","eval_increments = 100\n","real_data = NormSimData\n","\n","\n","# size of the latent space\n","latent_dim = 3 # doesn't need to be 3. could be any size. chose 3 to match output dimensions\n","\n","Inp_Outs = NormSimData.shape[1]\n","\n","# create the discriminator\n","discriminator = define_discriminator(Inp_Outs,AdamlearnRate)\n","\n","# create the generator\n","generator = define_generator(latent_dim,Inp_Outs)\n","\n","# create the gan\n","gan_model = define_gan(generator, discriminator,AdamlearnRate)\n","\n","# train model\n","train(generator, discriminator, gan_model, real_data, latent_dim,epochs,batches,eval_increments)\n","\n"],"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"8UUWXyAKIEW3","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"-src2UpkIExM","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"mFS_vKsKIFSH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"e1898b00-196a-4b6c-d5be-e66b0f193d75","executionInfo":{"status":"ok","timestamp":1571605467460,"user_tz":-660,"elapsed":447,"user":{"displayName":"Justin Hitchen","photoUrl":"","userId":"15321264705279366966"}}},"source":["\n","NumVars = NormMixedData.shape[1]\n","NormMixedData_Norm = NormMixedData[NormMixedData[:,3]==0]\n","NormMixedData_Anom = NormMixedData[NormMixedData[:,3]==1]\n","\n","x_mixed_Norm, y_mixed_Norm = NormMixedData_Norm[:,0:NumVars], NormMixedData_Norm[:,-1] \n","x_mixed_Anom, y_mixed_Anom = NormMixedData_Anom[:,0:NumVars], NormMixedData_Anom[:,-1] \n","\n","# evaluate discriminator on mixed (normal/anomaly) data sample\n","\n","_, acc_normal = discriminator.evaluate(x_mixed_Norm, y_mixed_Norm)\n","_, acc_anomaly = discriminator.evaluate(x_mixed_Anom, y_mixed_Anom) \n","print(\"D_Normal_acc: \", acc_normal)\n","print(\"D_IT_acc: \", acc_anomaly)\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["30000/30000 [==============================] - 1s 20us/step\n","20/20 [==============================] - 0s 48us/step\n","D_Normal_acc:  1.0\n","D_IT_acc:  0.949999988079071\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WT9ga-FWd5ls","colab_type":"text"},"source":["##Concluding Comments"]},{"cell_type":"markdown","metadata":{"id":"F4p1QYH9d99E","colab_type":"text"},"source":["This notebook has provided an example of how to implement a vanilla GAN structure. The GAN structure has been able to identify anomalous patterns in previously unseen data. \n","\n","<br>Insider Trading is notoriously challenging to identify. Whilst a vanilla GAN of the form implemented in this notebook may have some success at identifying insider trading, it is likely that a more sophisticated GAN capable at analysing sequences will be more effective. Therefore, we can extend the framework above to a **LSTM-GAN** framework. By altering the NN in the GAN to LSTM-NN, the model will be able to look at sequences of trading, rather than stand-alone observations. This will be valuable in identifying situations where inside traders attempt to obscure their trades by placing sequences of smaller trades."]}]}